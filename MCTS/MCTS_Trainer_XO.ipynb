{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from XO import *\n",
    "from MCTS_trainer import *\n",
    "\n",
    "t = Trainer(Environment,100,100)\n",
    "p, v = t.train(20,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), 'XO_policy_net.pth')\n",
    "torch.save(value_net.state_dict(), 'XO_value_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "plt.title(\"XO, Policy Network Losses\")\n",
    "plt.plot(np.arange(np.array(p).shape[0])+1,p)\n",
    "plt.xlabel(\"Batches\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"XO, Value Network Losses\")\n",
    "plt.plot(np.arange(np.array(v).shape[0])+1,v)\n",
    "plt.xlabel(\"Batches\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Environment()\n",
    "MC = MonteCarlo(board=game,num_simulations=1000)\n",
    "game = Environment()\n",
    "game.reset()\n",
    "\n",
    "for i in range(9):\n",
    "    print(\"---------- Player Move --------------\")\n",
    "    action = int(input())\n",
    "    _,reward,done,_ = game.take_action(action)\n",
    "    game.print()\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "    tree = MC.play_and_search(value_net,policy_net,game.game_state()[0],1,verbose=False)\n",
    "    #action_probs = t.policy_net.forward(torch.tensor(game.game_state()[0]).type(torch.FloatTensor).reshape(1,-1)).reshape(-1).detach().cpu().numpy()\n",
    "    \n",
    "    print(\"---------- Agent Move --------------\")\n",
    "    action_probs = np.zeros(9)\n",
    "    for action,child in tree.children.items():\n",
    "        action_probs[action] += child.visit_count # Use Value as well or not?\n",
    "\n",
    "    #action_probs = np.exp(action_probs/np.max(action_probs))\n",
    "    action_probs *= game.get_valid_actions()\n",
    "    action_probs /= np.sum(action_probs)\n",
    "    best_action = np.argmax(action_probs)\n",
    "    _,reward,done,_ = game.take_action(best_action)\n",
    "    game.print()\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
